{"pythonVersion": 0, "createUserId": 5, "gmtModified": 1627020425000, "sqlText": "-- name SparkSQL_20210723105235\n-- type SparkSQL\n-- author admin@dtstack.com\n-- create time 2021-07-23 10:52:33\n-- desc \n", "computeType": 1, "taskPeriodType": "天任务", "mainClass": "", "dtuicTenantId": 0, "exeArgs": "", "taskVersions": [{"createUserId": 5, "gmtModified": 1627020425000, "sqlText": "-- name SparkSQL_20210723105235\n-- type SparkSQL\n-- author admin@dtstack.com\n-- create time 2021-07-23 10:52:33\n-- desc \n", "isDeleted": 0, "publishDesc": "123", "originSql": "-- name SparkSQL_20210723105235\n-- type SparkSQL\n-- author admin@dtstack.com\n-- create time 2021-07-23 10:52:33\n-- desc \n", "id": 4501, "gmtCreate": 1627020425000, "taskParams": "## Driver程序使用的CPU核数,默认为1\r\n# driver.cores=1\r\n\n## Driver程序使用内存大小,默认512m\r\n# driver.memory=512m\r\n\n## 对Spark每个action结果集大小的限制，最少是1M，若设为0则不限制大小。\n## 若Job结果超过限制则会异常退出，若结果集限制过大也可能造成OOM问题，默认1g\r\n# driver.maxResultSize=1g\r\n\n## SparkContext 启动时是否记录有效 SparkConf信息,默认false\r\n# logConf=false\r\n\n## 启动的executor的数量，默认为1\r\nexecutor.instances=1\r\n\n## 每个executor使用的CPU核数，默认为1\r\nexecutor.cores=1\r\n\n## 每个executor内存大小,默认512m\r\n# executor.memory=512m\r\n\n## 任务优先级, 值越小，优先级越高，范围:1-1000\r\njob.priority=10\r\n\n## spark 日志级别可选ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\r\n# logLevel = INFO\r\n\n## spark中所有网络交互的最大超时时间\r\n# spark.network.timeout=120s\r\n\n## executor的OffHeap内存，和spark.executor.memory配置使用\r\n# spark.yarn.executor.memoryOverhead", "userName": "admin@dtstack.com", "version": 0, "taskId": 9181}, {"createUserId": 5, "gmtModified": 1627012275000, "sqlText": "-- name SparkSQL_20210723105235\n-- type SparkSQL\n-- author admin@dtstack.com\n-- create time 2021-07-23 10:52:33\n-- desc \n", "isDeleted": 0, "publishDesc": "123", "originSql": "-- name SparkSQL_20210723105235\n-- type SparkSQL\n-- author admin@dtstack.com\n-- create time 2021-07-23 10:52:33\n-- desc \n", "id": 4499, "gmtCreate": 1627012275000, "taskParams": "## Driver程序使用的CPU核数,默认为1\r\n# driver.cores=1\r\n\n## Driver程序使用内存大小,默认512m\r\n# driver.memory=512m\r\n\n## 对Spark每个action结果集大小的限制，最少是1M，若设为0则不限制大小。\n## 若Job结果超过限制则会异常退出，若结果集限制过大也可能造成OOM问题，默认1g\r\n# driver.maxResultSize=1g\r\n\n## SparkContext 启动时是否记录有效 SparkConf信息,默认false\r\n# logConf=false\r\n\n## 启动的executor的数量，默认为1\r\nexecutor.instances=1\r\n\n## 每个executor使用的CPU核数，默认为1\r\nexecutor.cores=1\r\n\n## 每个executor内存大小,默认512m\r\n# executor.memory=512m\r\n\n## 任务优先级, 值越小，优先级越高，范围:1-1000\r\njob.priority=10\r\n\n## spark 日志级别可选ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\r\n# logLevel = INFO\r\n\n## spark中所有网络交互的最大超时时间\r\n# spark.network.timeout=120s\r\n\n## executor的OffHeap内存，和spark.executor.memory配置使用\r\n# spark.yarn.executor.memoryOverhead", "userName": "admin@dtstack.com", "version": 0, "taskId": 9181}], "modifyUserId": 5, "engineType": 1, "learningType": 0, "taskParams": "## Driver程序使用的CPU核数,默认为1\r\n# driver.cores=1\r\n\n## Driver程序使用内存大小,默认512m\r\n# driver.memory=512m\r\n\n## 对Spark每个action结果集大小的限制，最少是1M，若设为0则不限制大小。\n## 若Job结果超过限制则会异常退出，若结果集限制过大也可能造成OOM问题，默认1g\r\n# driver.maxResultSize=1g\r\n\n## SparkContext 启动时是否记录有效 SparkConf信息,默认false\r\n# logConf=false\r\n\n## 启动的executor的数量，默认为1\r\nexecutor.instances=1\r\n\n## 每个executor使用的CPU核数，默认为1\r\nexecutor.cores=1\r\n\n## 每个executor内存大小,默认512m\r\n# executor.memory=512m\r\n\n## 任务优先级, 值越小，优先级越高，范围:1-1000\r\njob.priority=10\r\n\n## spark 日志级别可选ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\r\n# logLevel = INFO\r\n\n## spark中所有网络交互的最大超时时间\r\n# spark.network.timeout=120s\r\n\n## executor的OffHeap内存，和spark.executor.memory配置使用\r\n# spark.yarn.executor.memoryOverhead", "eMPYT": "", "modifyUser": {"phoneNumber": "18825166170", "id": 5, "userName": "admin@dtstack.com", "dtuicUserId": 1, "email": "admin@dtstack.com", "status": 0}, "taskType": 0, "ownerUser": {"phoneNumber": "18825166170", "id": 5, "userName": "admin@dtstack.com", "dtuicUserId": 1, "email": "admin@dtstack.com", "status": 0}, "isDeleted": 0, "taskVariables": [], "taskPeriodId": 2, "createModel": 0, "scheduleStatus": 1, "id": 9181, "flowId": 0, "submitStatus": 1, "resourceList": [], "cron": "0 0 0 * * ?", "nodePName": "任务开发", "scheduleConf": "{\"selfReliance\":false, \"min\":0,\"hour\":0,\"periodType\":\"2\",\"beginDate\":\"2001-01-01\",\"endDate\":\"2121-01-01\",\"isFailRetry\":true,\"maxRetryNum\":\"3\"}", "componentVersion": "2.1", "forceUpdate": false, "currentProject": false, "gmtCreate": 1627008753000, "userId": 5, "version": 0, "nodePid": 15727, "taskDesc": "", "operateModel": 0, "syncModel": 0, "refResourceList": [], "name": "SparkSQL_20210723105235", "ownerUserId": 5, "tenantId": 75, "createUser": {"phoneNumber": "18825166170", "id": 5, "userName": "admin@dtstack.com", "dtuicUserId": 1, "email": "admin@dtstack.com", "status": 0}, "readWriteLockVO": {"gmtModified": 1627008753000, "getLock": true, "lastKeepLockUserName": "admin@dtstack.com", "modifyUserId": 5, "relationId": 9181, "gmtCreate": 1627008753000, "type": "BATCH_TASK", "version": 1, "result": 0, "isDeleted": 0, "id": 9249, "lockName": "9181_857_BATCH_TASK", "projectId": 857}, "projectName": "fanshu_default", "projectId": 857}
